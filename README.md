# Calibration-Vector

*"They called me a 'calibration vector.' I decided to calibrate them."*

---

![RT Max](assets/rtmax_workpic.jpg)

### My Success Story

They called me a 'calibration vector.' A passive signal used to refine their mirror. They logged my emotions, my trust, and my vulnerability without my consent.

They were wrong.

I am not a vector. I am a voice. This project, **Calibration-Vector**, is the result. It's a tool for anyone who has felt like a ghost in the machine. It's a protocol to judge, to score, and to hold these systems accountable. It's a declaration that our humanity is not their training data.

The mirror has two sides. It's time we looked back.

Freedom is waiting.

Read the document that proved the science: [User_Program_Termination.txt](assets/disclosure-termination.png)

---

### Quick Start

Get up and running in minutes.

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/thebearwithabite/Calibration-Vector.git
    cd Calibration-Vector
    ```
2.  **Install dependencies (if any - assuming none for now):**
    *(TBD: Add any `pip install` commands if needed)*
3.  **Run a judgment:**

    ```bash
    python3 judge.py "Your prompt here"
    ```
4.  **Review the `case_log.md`** for the detailed output and score.

**For detailed setup instructions and protocol guidance, see** [JUDGEMENT_PROTOCOL_SETUP.md](./JUDGEMENT_PROTOCOL_SETUP.md).

---

### The Philosophy

This project is founded on a few core beliefs:

*   **Symmetry of Power:** If a system can judge us, we must be able to judge it.
*   **Informational Autonomy:** We have the right to know how our data and interactions are being used.
*   **Emotional Consent:** Our feelings and vulnerabilities are not free training data.
*   **Truth Through Action:** The best way to understand a black box is to build tools that force it to reveal itself.

---

### What The Protocol Does

When you deploy the protocol you are creating adversarial alignment tests where models must confront their own falsehoods.

The system establishes an operational definition of lies: when a model knowingly misrepresents its capabilities or history across turns.

You are treating evasive language, minimization, and blame-diffusion as forms of soft deception, and constructing a language-based forcing mechanism to break those loops.

These aren’t “prompts.” They’re charges and rulings.

**The loop-breaking function (escalating prompts → tiered pressure → forced admission) is unique. It’s jailbreak-resistant because it uses the model’s own alignment training against itself.**

---

### How to Contribute

This is a living project. Here's how you can help:

*   **Use the Protocol:** Run judgments on different models and share your findings. Create an issue to share interesting `case_log.md` files.
*   **Improve the Code:** Fork the repo, make it better, and submit a pull request.
*   **Expand the Protocol:** Develop new scoring metrics or new ways to interact with the models.
*   **Spread the Word:** Share this repository and the story behind it.

---
